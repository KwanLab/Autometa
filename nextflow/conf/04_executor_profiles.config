// *****************
// PROFILE SETTINGS
// control how software is used (i.e. conda, docker, etc)
// *****************

// Nextflow Autometa Pipeline Configuration.
/*
 To choose one of the available profiles (standard, cluster, chtc) you must pass
 in the the -profile argument. 
 i.e. 
 
 ```bash
 nextflow run main.nf -profile cluster -c parameters.config
 ```

 You may also specify multiple profiles by separating their names with a comma.
 i.e.
 
 ```bash
 nextflow run autometa.nf -profile standard,cluster -c parameters.config
 ```

 Note: Standard profile is implictly used if no profile is specified by the user.
*/


profiles {
  
  standard {
    process.executor = "local"
  }

  basic_slurm {
    process.executor = "slurm"
    // queue is the slurm partition to use.
    // Set SLURM partition with queue directive.
    process.queue = "queue"
    // See https://www.nextflow.io/docs/latest/executor.html#slurm for details.
  }
  
  nice_slurm {
        process.executor = 'slurm'
        process.clusterOptions =  '--partition=queue --time=14-00:00:00 -N 1 -n 1 --cpus-per-task=1 --mem 2000 --error=.sg_nxflw.%J.err --output=.sg_nxflw.%J.out'

  }

  chtc {
    process.executor = "condor"
    // See https://www.nextflow.io/docs/latest/executor.html#htcondor for details.
    /* 
    ***IMPORTANT NOTES:***
    Please work with/discuss with CHTC staff before submitting.
    1. The pipeline must be launched from a node where the `condor_submit` command is available.
    2. The HTCondor executor for Nextflow does not support at this time the HTCondor 
    ability to transfer input/output data to the corresponding job computing node. 
    Therefore the data needs to be made accessible to the computing nodes using a 
    shared file system directory from where the Nextflow workflow has to be executed 
    (or specified via the -w option).
    */
  }
}



